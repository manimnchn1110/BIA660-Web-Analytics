{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook try to learn from information_extraction.py\n",
    "\n",
    "- import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import re\n",
    "import spacy\n",
    "from pyclausie import ClausIE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- automatically download the clausie jar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = ClausIE.get_instance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load English in module spacy.language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load pattern object fun  \n",
    "        compile(pattern, flags=0)\n",
    "        Compile a regular expression pattern, returning a pattern object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_spaces = re.compile(r'\\s+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define Person class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person(object):\n",
    "    def __init__(self, name, likes=None, has=None, travels=None):\n",
    "        #Person's name\n",
    "        self.name = name\n",
    "        #If there is a \"likes\" in the sentence, store it in the list otherwise store nothing\n",
    "        self.likes = [] if likes is None else likes  \n",
    "        #If there is a \"has\" in the sentence, store it in the list, otherwise store nothing\n",
    "        self.has = [] if has is None else has\n",
    "        #If there is a \"travels\" in the sentence, store it in the list, otherwise store nothing\n",
    "        self.travels = [] if travels is None else travels\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.name\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define Pet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pet(object):\n",
    "    def __init__(self, pet_type, name=None):\n",
    "        #Pet's name\n",
    "        self.name = name\n",
    "        #Pet's type\n",
    "        self.type = pet_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define Trip class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trip(object):\n",
    "    def __init__(self):\n",
    "        # departure time\n",
    "        self.departs_on = None\n",
    "        # departure destination\n",
    "        self.departs_to = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create empty lists to collect persons, pets and trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = []\n",
    "pets = []\n",
    "trips = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define a function read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Joe likes Mary.',\n",
       " 'Bob is friends with Mary.',\n",
       " 'Bob has a dog named Fido.',\n",
       " 'Mary has a dog.',\n",
       " \"Mary's dog's name is Rover.\",\n",
       " \"Bob doesn't like Joe.\",\n",
       " 'Joe is friends with Mary.',\n",
       " 'Joe has a cat.',\n",
       " \"Joe's cat's name is Mr. Binglesworth.\",\n",
       " 'Bob and Mary are taking a trip to France in June of this year.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_data_from_file(file_path):\n",
    "    #read data and ignore data with abnormal value\n",
    "    with open(file_path) as infile:\n",
    "        cleaned_lines = [line.strip() for line in infile if not line.startswith(('$$$', '###', '==='))]\n",
    "    #return these values with cleaning\n",
    "    return cleaned_lines\n",
    "\n",
    "data_sample = get_data_from_file('./chatbot_data.txt')\n",
    "data_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define a select person function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_person(name):\n",
    "    #If input name is one person's name who is in the persons list, return it.\n",
    "    for person in persons:\n",
    "        if person.name == name:\n",
    "            return person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define a function add person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_person(name):\n",
    "    #apply select_person function\n",
    "    #if input name is one person's name who is in the list already.\n",
    "    #return it.\n",
    "    person = select_person(name)\n",
    "    if person is None:\n",
    "        new_person = Person(name)\n",
    "        persons.apend(new_person)\n",
    "        return new_person\n",
    "    #if there is not such person, \n",
    "    #define it as a new person\n",
    "    #and add it into the persons list\n",
    "    #then return this new person\n",
    "    return person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define a function select pet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_pet(name):\n",
    "    for pet in person:#????\n",
    "        if pet.name == name:\n",
    "            return pet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- define a function add pet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pet(type, name=None):\n",
    "    #refresh variable pet\n",
    "    pet = None\n",
    "    #if name is ture\n",
    "    #select this pet from persons list\n",
    "    #update variable pet\n",
    "    if name:\n",
    "        pet = select_pet(name)\n",
    "    #then pet = to \n",
    "    if pet is None:\n",
    "        pet = Pet(type, name)\n",
    "        pets.append(pet)\n",
    "\n",
    "    return pet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define a function persons has pets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_persons_pet(person_name):\n",
    "    #apply select_person function\n",
    "    #choose one person in persons list with his or her name\n",
    "    person = select_person(person_name)\n",
    "    #for any thing in the person.has list\n",
    "    for thing in person.has:\n",
    "        #test if thing is pet\n",
    "        #if yes, return thing\n",
    "        if isinstance(thing, Pet):\n",
    "            return thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define extract subject, object verb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Test extract triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triple(index='1', subject='Joe', predicate='likes', object='Mary')\n",
      "Triple(index='2', subject='Bob', predicate='is', object='friends with Mary')\n",
      "Triple(index='2', subject='Bob', predicate='is', object='friends')\n",
      "Triple(index='3', subject='Bob', predicate='has', object='a dog named Fido')\n",
      "Triple(index='3', subject='a dog', predicate='be named', object='Fido')\n",
      "Triple(index='4', subject='Mary', predicate='has', object='a dog')\n",
      "Triple(index='5', subject='Mary', predicate='has', object=\"dog 's\")\n",
      "Triple(index='5', subject=\"Mary 's dog\", predicate='has', object='name')\n",
      "Triple(index='5', subject=\"Mary 's dog 's name\", predicate='is', object='Rover')\n",
      "Triple(index='6', subject='Bob', predicate=\"does n't like\", object='Joe')\n",
      "Triple(index='7', subject='Joe', predicate='is', object='friends with Mary')\n",
      "Triple(index='7', subject='Joe', predicate='is', object='friends')\n",
      "Triple(index='8', subject='Joe', predicate='has', object='a cat')\n",
      "Triple(index='9', subject='Joe', predicate='has', object=\"cat 's\")\n",
      "Triple(index='9', subject=\"Joe 's cat\", predicate='has', object='name')\n",
      "Triple(index='9', subject=\"Joe 's cat 's name\", predicate='is', object='Mr. Binglesworth')\n",
      "Triple(index='10', subject='Bob and Mary', predicate='are taking', object='a trip to France in June of this year')\n",
      "subject: Joe predicate: likes object: Mary\n",
      "subject: Bob predicate: is object: friends with Mary\n",
      "subject: Bob predicate: is object: friends\n",
      "subject: Bob predicate: has object: a dog named Fido\n",
      "subject: a dog predicate: be named object: Fido\n",
      "subject: Mary predicate: has object: a dog\n",
      "subject: Mary predicate: has object: dog 's\n",
      "subject: Mary 's dog predicate: has object: name\n",
      "subject: Mary 's dog 's name predicate: is object: Rover\n",
      "subject: Bob predicate: does n't like object: Joe\n",
      "subject: Joe predicate: is object: friends with Mary\n",
      "subject: Joe predicate: is object: friends\n",
      "subject: Joe predicate: has object: a cat\n",
      "subject: Joe predicate: has object: cat 's\n",
      "subject: Joe 's cat predicate: has object: name\n",
      "subject: Joe 's cat 's name predicate: is object: Mr. Binglesworth\n",
      "subject: Bob and Mary predicate: are taking object: a trip to France in June of this year\n"
     ]
    }
   ],
   "source": [
    "sents = data_sample\n",
    "triples = cl.extract_triples(sents)\n",
    "\n",
    "for triple in triples:\n",
    "    print (triple)\n",
    "    \n",
    "for triple in triples:\n",
    "    print (\"subject: \" + triple.subject + \" predicate: \" + triple.predicate + \" object: \" + triple.object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test doc/nlp/unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob and Mary are taking a trip to France in June of this year.\n",
      "Bob and Mary are taking a trip to France in June of this year.\n",
      "(Bob, Mary, France, June of this year)\n",
      "Bob PROPN taking bob nsubj\n",
      "and CCONJ Bob and cc\n",
      "Mary PROPN Bob mary conj\n",
      "are VERB taking be aux\n",
      "taking VERB taking take ROOT\n",
      "a DET trip a det\n",
      "trip NOUN taking trip dobj\n",
      "to ADP trip to prep\n",
      "France PROPN to france pobj\n",
      "in ADP taking in prep\n",
      "June PROPN in june pobj\n",
      "of ADP June of prep\n",
      "this DET year this det\n",
      "year NOUN of year pobj\n",
      ". PUNCT taking . punct\n"
     ]
    }
   ],
   "source": [
    "ori = sents[9]\n",
    "print(ori)\n",
    "doc = nlp(unicode(sents[9]))\n",
    "print(doc)\n",
    "print(doc.ents)\n",
    "for i in doc:\n",
    "    print (i, i.pos_,i.head, i.lemma_, i.dep_)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Token in spacy.tokens.token:\n",
      "\n",
      "spacy.tokens.token.Token = class Token(__builtin__.object)\n",
      " |  An individual token â€“ i.e. a word, punctuation symbol, whitespace,\n",
      " |  etc.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __bytes__(...)\n",
      " |  \n",
      " |  __eq__(...)\n",
      " |      x.__eq__(y) <==> x==y\n",
      " |  \n",
      " |  __ge__(...)\n",
      " |      x.__ge__(y) <==> x>=y\n",
      " |  \n",
      " |  __gt__(...)\n",
      " |      x.__gt__(y) <==> x>y\n",
      " |  \n",
      " |  __hash__(...)\n",
      " |      x.__hash__() <==> hash(x)\n",
      " |  \n",
      " |  __le__(...)\n",
      " |      x.__le__(y) <==> x<=y\n",
      " |  \n",
      " |  __len__(...)\n",
      " |      The number of unicode characters in the token, i.e. `token.text`.\n",
      " |      \n",
      " |      RETURNS (int): The number of unicode characters in the token.\n",
      " |  \n",
      " |  __lt__(...)\n",
      " |      x.__lt__(y) <==> x<y\n",
      " |  \n",
      " |  __ne__(...)\n",
      " |      x.__ne__(y) <==> x!=y\n",
      " |  \n",
      " |  __reduce__ = __reduce_cython__(...)\n",
      " |  \n",
      " |  __repr__(...)\n",
      " |      x.__repr__() <==> repr(x)\n",
      " |  \n",
      " |  __setstate__ = __setstate_cython__(...)\n",
      " |  \n",
      " |  __str__(...)\n",
      " |      x.__str__() <==> str(x)\n",
      " |  \n",
      " |  __unicode__(...)\n",
      " |  \n",
      " |  check_flag(...)\n",
      " |      Check the value of a boolean flag.\n",
      " |      \n",
      " |      flag_id (int): The ID of the flag attribute.\n",
      " |      RETURNS (bool): Whether the flag is set.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> from spacy.attrs import IS_TITLE\n",
      " |          >>> doc = nlp(u'Give it back! He pleaded.')\n",
      " |          >>> token = doc[0]\n",
      " |          >>> token.check_flag(IS_TITLE)\n",
      " |          True\n",
      " |  \n",
      " |  get_extension(...)\n",
      " |  \n",
      " |  has_extension(...)\n",
      " |  \n",
      " |  is_ancestor(...)\n",
      " |      Check whether this token is a parent, grandparent, etc. of another\n",
      " |      in the dependency tree.\n",
      " |      \n",
      " |      descendant (Token): Another token.\n",
      " |      RETURNS (bool): Whether this token is the ancestor of the descendant.\n",
      " |  \n",
      " |  nbor(...)\n",
      " |      Get a neighboring token.\n",
      " |      \n",
      " |      i (int): The relative position of the token to get. Defaults to 1.\n",
      " |      RETURNS (Token): The token at position `self.doc[self.i+i]`.\n",
      " |  \n",
      " |  set_extension(...)\n",
      " |  \n",
      " |  similarity(...)\n",
      " |      Make a semantic similarity estimate. The default estimate is cosine\n",
      " |      similarity using an average of word vectors.\n",
      " |      \n",
      " |      other (object): The object to compare with. By default, accepts `Doc`,\n",
      " |          `Span`, `Token` and `Lexeme` objects.\n",
      " |      RETURNS (float): A scalar similarity score. Higher is more similar.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  ancestors\n",
      " |      A sequence of this token's syntactic ancestors.\n",
      " |      \n",
      " |      YIELDS (Token): A sequence of ancestor tokens such that\n",
      " |          `ancestor.is_ancestor(self)`.\n",
      " |  \n",
      " |  children\n",
      " |      A sequence of the token's immediate syntactic children.\n",
      " |      \n",
      " |      YIELDS (Token): A child token such that child.head==self\n",
      " |  \n",
      " |  cluster\n",
      " |      RETURNS (int): Brown cluster ID.\n",
      " |  \n",
      " |  conjuncts\n",
      " |      A sequence of coordinated tokens, including the token itself.\n",
      " |      \n",
      " |      YIELDS (Token): A coordinated token.\n",
      " |  \n",
      " |  dep\n",
      " |      RETURNS (uint64): ID of syntactic dependency label.\n",
      " |  \n",
      " |  dep_\n",
      " |      RETURNS (unicode): The syntactic dependency label.\n",
      " |  \n",
      " |  doc\n",
      " |  \n",
      " |  ent_id\n",
      " |      RETURNS (uint64): ID of the entity the token is an instance of,\n",
      " |      if any.\n",
      " |  \n",
      " |  ent_id_\n",
      " |      RETURNS (unicode): ID of the entity the token is an instance of,\n",
      " |      if any.\n",
      " |  \n",
      " |  ent_iob\n",
      " |      IOB code of named entity tag. `1=\"I\", 2=\"O\", 3=\"B\"`. 0 means no tag\n",
      " |      is assigned.\n",
      " |      \n",
      " |      RETURNS (uint64): IOB code of named entity tag.\n",
      " |  \n",
      " |  ent_iob_\n",
      " |      IOB code of named entity tag. \"B\" means the token begins an entity,\n",
      " |      \"I\" means it is inside an entity, \"O\" means it is outside an entity,\n",
      " |      and \"\" means no entity tag is set.\n",
      " |      \n",
      " |      RETURNS (unicode): IOB code of named entity tag.\n",
      " |  \n",
      " |  ent_type\n",
      " |      RETURNS (uint64): Named entity type.\n",
      " |  \n",
      " |  ent_type_\n",
      " |      RETURNS (unicode): Named entity type.\n",
      " |  \n",
      " |  has_vector\n",
      " |      A boolean value indicating whether a word vector is associated with\n",
      " |      the object.\n",
      " |      \n",
      " |      RETURNS (bool): Whether a word vector is associated with the object.\n",
      " |  \n",
      " |  head\n",
      " |      The syntactic parent, or \"governor\", of this token.\n",
      " |      \n",
      " |      RETURNS (Token): The token predicted by the parser to be the head of\n",
      " |          the current token.\n",
      " |  \n",
      " |  i\n",
      " |  \n",
      " |  idx\n",
      " |      RETURNS (int): The character offset of the token within the parent\n",
      " |      document.\n",
      " |  \n",
      " |  is_alpha\n",
      " |      RETURNS (bool): Whether the token consists of alpha characters.\n",
      " |      Equivalent to `token.text.isalpha()`.\n",
      " |  \n",
      " |  is_ascii\n",
      " |      RETURNS (bool): Whether the token consists of ASCII characters.\n",
      " |      Equivalent to `[any(ord(c) >= 128 for c in token.text)]`.\n",
      " |  \n",
      " |  is_bracket\n",
      " |      RETURNS (bool): Whether the token is a bracket.\n",
      " |  \n",
      " |  is_digit\n",
      " |      RETURNS (bool): Whether the token consists of digits. Equivalent to\n",
      " |      `token.text.isdigit()`.\n",
      " |  \n",
      " |  is_left_punct\n",
      " |      RETURNS (bool): Whether the token is a left punctuation mark.\n",
      " |  \n",
      " |  is_lower\n",
      " |      RETURNS (bool): Whether the token is in lowercase. Equivalent to\n",
      " |      `token.text.islower()`.\n",
      " |  \n",
      " |  is_oov\n",
      " |      RETURNS (bool): Whether the token is out-of-vocabulary.\n",
      " |  \n",
      " |  is_punct\n",
      " |      RETURNS (bool): Whether the token is punctuation.\n",
      " |  \n",
      " |  is_quote\n",
      " |      RETURNS (bool): Whether the token is a quotation mark.\n",
      " |  \n",
      " |  is_right_punct\n",
      " |      RETURNS (bool): Whether the token is a left punctuation mark.\n",
      " |  \n",
      " |  is_sent_start\n",
      " |      RETURNS (bool / None): Whether the token starts a sentence.\n",
      " |      None if unknown.\n",
      " |  \n",
      " |  is_space\n",
      " |      RETURNS (bool): Whether the token consists of whitespace characters.\n",
      " |      Equivalent to `token.text.isspace()`.\n",
      " |  \n",
      " |  is_stop\n",
      " |      RETURNS (bool): Whether the token is a stop word, i.e. part of a\n",
      " |      \"stop list\" defined by the language data.\n",
      " |  \n",
      " |  is_title\n",
      " |      RETURNS (bool): Whether the token is in titlecase. Equivalent to\n",
      " |      `token.text.istitle()`.\n",
      " |  \n",
      " |  is_upper\n",
      " |      RETURNS (bool): Whether the token is in uppercase. Equivalent to\n",
      " |      `token.text.isupper()`\n",
      " |  \n",
      " |  lang\n",
      " |      RETURNS (uint64): ID of the language of the parent document's\n",
      " |      vocabulary.\n",
      " |  \n",
      " |  lang_\n",
      " |      RETURNS (unicode): Language of the parent document's vocabulary,\n",
      " |      e.g. 'en'.\n",
      " |  \n",
      " |  left_edge\n",
      " |      The leftmost token of this token's syntactic descendents.\n",
      " |      \n",
      " |      RETURNS (Token): The first token such that `self.is_ancestor(token)`.\n",
      " |  \n",
      " |  lefts\n",
      " |      The leftward immediate children of the word, in the syntactic\n",
      " |      dependency parse.\n",
      " |      \n",
      " |      YIELDS (Token): A left-child of the token.\n",
      " |  \n",
      " |  lemma\n",
      " |      RETURNS (uint64): ID of the base form of the word, with no\n",
      " |      inflectional suffixes.\n",
      " |  \n",
      " |  lemma_\n",
      " |      RETURNS (unicode): The token lemma, i.e. the base form of the word,\n",
      " |      with no inflectional suffixes.\n",
      " |  \n",
      " |  lex_id\n",
      " |      RETURNS (int): Sequential ID of the token's lexical type.\n",
      " |  \n",
      " |  like_email\n",
      " |      RETURNS (bool): Whether the token resembles an email address.\n",
      " |  \n",
      " |  like_num\n",
      " |      RETURNS (bool): Whether the token resembles a number, e.g. \"10.9\",\n",
      " |      \"10\", \"ten\", etc.\n",
      " |  \n",
      " |  like_url\n",
      " |      RETURNS (bool): Whether the token resembles a URL.\n",
      " |  \n",
      " |  lower\n",
      " |      RETURNS (uint64): ID of the lowercase token text.\n",
      " |  \n",
      " |  lower_\n",
      " |      RETURNS (unicode): The lowercase token text. Equivalent to\n",
      " |      `Token.text.lower()`.\n",
      " |  \n",
      " |  n_lefts\n",
      " |      RETURNS (int): The number of leftward immediate children of the\n",
      " |      word, in the syntactic dependency parse.\n",
      " |  \n",
      " |  n_rights\n",
      " |      RETURNS (int): The number of rightward immediate children of the\n",
      " |      word, in the syntactic dependency parse.\n",
      " |  \n",
      " |  norm\n",
      " |      RETURNS (uint64): ID of the token's norm, i.e. a normalised form of\n",
      " |      the token text. Usually set in the language's tokenizer exceptions\n",
      " |      or norm exceptions.\n",
      " |  \n",
      " |  norm_\n",
      " |      RETURNS (unicode): The token's norm, i.e. a normalised form of the\n",
      " |      token text. Usually set in the language's tokenizer exceptions or\n",
      " |      norm exceptions.\n",
      " |  \n",
      " |  orth\n",
      " |      RETURNS (uint64): ID of the verbatim text content.\n",
      " |  \n",
      " |  orth_\n",
      " |      RETURNS (unicode): Verbatim text content (identical to\n",
      " |      `Token.text`). Existst mostly for consistency with the other\n",
      " |      attributes.\n",
      " |  \n",
      " |  pos\n",
      " |      RETURNS (uint64): ID of coarse-grained part-of-speech tag.\n",
      " |  \n",
      " |  pos_\n",
      " |      RETURNS (unicode): Coarse-grained part-of-speech tag.\n",
      " |  \n",
      " |  prefix\n",
      " |      RETURNS (uint64): ID of a length-N substring from the start of the\n",
      " |      token. Defaults to `N=1`.\n",
      " |  \n",
      " |  prefix_\n",
      " |      RETURNS (unicode): A length-N substring from the start of the token.\n",
      " |      Defaults to `N=1`.\n",
      " |  \n",
      " |  prob\n",
      " |      RETURNS (float): Smoothed log probability estimate of token type.\n",
      " |  \n",
      " |  rank\n",
      " |      RETURNS (int): Sequential ID of the token's lexical type, used to\n",
      " |      index into tables, e.g. for word vectors.\n",
      " |  \n",
      " |  right_edge\n",
      " |      The rightmost token of this token's syntactic descendents.\n",
      " |      \n",
      " |      RETURNS (Token): The last token such that `self.is_ancestor(token)`.\n",
      " |  \n",
      " |  rights\n",
      " |      The rightward immediate children of the word, in the syntactic\n",
      " |      dependency parse.\n",
      " |      \n",
      " |      YIELDS (Token): A right-child of the token.\n",
      " |  \n",
      " |  sent_start\n",
      " |  \n",
      " |  sentiment\n",
      " |      RETURNS (float): A scalar value indicating the positivity or\n",
      " |      negativity of the token.\n",
      " |  \n",
      " |  shape\n",
      " |      RETURNS (uint64): ID of the token's shape, a transform of the\n",
      " |      tokens's string, to show orthographic features (e.g. \"Xxxx\", \"dd\").\n",
      " |  \n",
      " |  shape_\n",
      " |      RETURNS (unicode): Transform of the tokens's string, to show\n",
      " |      orthographic features. For example, \"Xxxx\" or \"dd\".\n",
      " |  \n",
      " |  string\n",
      " |      Deprecated: Use Token.text_with_ws instead.\n",
      " |  \n",
      " |  subtree\n",
      " |      A sequence of all the token's syntactic descendents.\n",
      " |      \n",
      " |      YIELDS (Token): A descendent token such that\n",
      " |          `self.is_ancestor(descendent)`.\n",
      " |  \n",
      " |  suffix\n",
      " |      RETURNS (uint64): ID of a length-N substring from the end of the\n",
      " |      token. Defaults to `N=3`.\n",
      " |  \n",
      " |  suffix_\n",
      " |      RETURNS (unicode): A length-N substring from the end of the token.\n",
      " |      Defaults to `N=3`.\n",
      " |  \n",
      " |  tag\n",
      " |      RETURNS (uint64): ID of fine-grained part-of-speech tag.\n",
      " |  \n",
      " |  tag_\n",
      " |      RETURNS (unicode): Fine-grained part-of-speech tag.\n",
      " |  \n",
      " |  text\n",
      " |      RETURNS (unicode): The original verbatim text of the token.\n",
      " |  \n",
      " |  text_with_ws\n",
      " |      RETURNS (unicode): The text content of the span (with trailing\n",
      " |      whitespace).\n",
      " |  \n",
      " |  vector\n",
      " |      A real-valued meaning representation.\n",
      " |      \n",
      " |      RETURNS (numpy.ndarray[ndim=1, dtype='float32']): A 1D numpy array\n",
      " |          representing the token's semantics.\n",
      " |  \n",
      " |  vector_norm\n",
      " |      The L2 norm of the token's vector representation.\n",
      " |      \n",
      " |      RETURNS (float): The L2 norm of the vector representation.\n",
      " |  \n",
      " |  vocab\n",
      " |  \n",
      " |  whitespace_\n",
      " |      RETURNS (unicode): The trailing whitespace character, if present.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __new__ = <built-in method __new__ of type object>\n",
      " |      T.__new__(S, ...) -> a new object with type S, a subtype of T\n",
      " |  \n",
      " |  __pyx_vtable__ = <capsule object NULL>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help('spacy.tokens.token.Token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_relation_triplet(triplet):\n",
    "    \n",
    "    \"\"\"\n",
    "    find relations of types:\n",
    "    (PERSON, likes, PERSON)\n",
    "    (PERSON, has, PET)\n",
    "    (PET, has_name, NAME)\n",
    "    (PERSON, travels, TRIP)\n",
    "    (TRIP, departs_on, DATE)\n",
    "    (TRIP, departs_to, PLACE)\n",
    "\n",
    "    :param triplet: The relation triplet from ClausIE\n",
    "    :type triplet: tuple\n",
    "    :return: a triplet in the formats specified above\n",
    "    :rtype: tuple\n",
    "    \"\"\"\n",
    "    #triplet = class Triple in ClausIE, Triple(index, subject, predicate, object)\n",
    "    #simple sentence: s + p + o\n",
    "    sentence = triplet.subject + ' ' + triplet.predicate + ' ' + triplet.object\n",
    "    \n",
    "    #help(unicode())\n",
    "    #unicode is a class in spacy with multiple powerful functions\n",
    "    #unicode() could convert string into unicode object\n",
    "    #sentence ==> unicode\n",
    "    \n",
    "    #help(nlp(unicode()))\n",
    "    #nlp(unicode()) : change unicode ==> Doc + apply 'en'\n",
    "    \"\"\"\"\"\n",
    "    Doc:A sequence of Token objects. Access sentences and named entities, export \n",
    "    annotations to numpy arrays, losslessly serialize to compressed binary strings.\n",
    "    \"\"\"\"\n",
    "    doc = nlp(unicode(sentence))\n",
    "    \n",
    "    #help('spacy.tokens.token.Token')\n",
    "    #Class token - multiple tokens = doc <== 'en' <== unicode <==sentence string\n",
    "    for token in doc:\n",
    "        \"\"\"\"\"\n",
    "        pos_: RETURNS (unicode): Coarse-grained part-of-speech tag.\n",
    "            \n",
    "            Examples:\n",
    "            Bob    and    Mary   are   taking a    trip  to   France in   June   of   this year  .\n",
    "            PROPN  CCONJ  PROPN  VERB  VERB   DET  NOUN  ADP  PROPN  ADP  PROPN  ADP  DET  NOUN PUNCT\n",
    "        \n",
    "        \n",
    "        head: The syntactic parent, or \"governor\", of this token. \n",
    "              RETURNS (Token): The token predicted by the parser to be the head of the current token.\n",
    "            \n",
    "            Examples:\n",
    "            Bob    and Mary are    taking a    trip   to   France in     June of   this year .\n",
    "            taking Bob Bob  taking taking trip taking trip to     taking in   June year of   taking\n",
    "        \n",
    "        \"\"\"\"\" \n",
    "        #verb and Notional verbs -> root\n",
    "        if token.pos_ == 'VERB' and token.head == token:\n",
    "            root = token\n",
    "            \n",
    "        # elif t.pos_ == 'NOUN'\n",
    "        # also, if only one sentence\n",
    "        # root = doc[:].root\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    CURRENT ASSUMPTIONS:\n",
    "    - People's names are unique (i.e. there only exists one person with a certain name).\n",
    "    - Pet's names are unique\n",
    "    - The only pets are dogs and cats\n",
    "    - Only one person can own a specific pet\n",
    "    - A person can own only one pet\n",
    "    \"\"\"\n",
    "\n",
    "# Process (PERSON, likes, PERSON) relations\n",
    "\n",
    "        \"\"\"\"\"\n",
    "        lemma_: RETURNS (unicode): The token lemma, i.e. the base form of the word, with no inflectional suffixes.\n",
    "        Bob bob\n",
    "        and and\n",
    "        Mary mary\n",
    "        are  be\n",
    "        taking take\n",
    "        a  a\n",
    "        trip trip\n",
    "        to  to\n",
    "        France france\n",
    "        in in\n",
    "        June june\n",
    "        of of\n",
    "        this this\n",
    "        year year\n",
    "        .  .\n",
    "        \"\"\"\"\n",
    "    #if verb, Notional verbs, base form is 'like' \n",
    "    #and if subjet is person, object is person\n",
    "    #add subjet to persons list\n",
    "    #add object to persons list\n",
    "    #store info in s (Class person: person(name, likes=[], has=[], travels=[]))\n",
    "    if root.lemma_ == 'like':\n",
    "        if triplet.subject in [e.text for e in doc.ents if e.label_ == 'PERSON'] and triplet.object in [e.text for e in doc.ents if e.label_ == 'PERSON']:\n",
    "            s = add_person(triplet.subject)\n",
    "            o = add_person(triplet.object)\n",
    "            s.likes.append(o)\n",
    "            \n",
    "    #if verb, Notional verbs, base form is 'be'\n",
    "    #object ==> unicode ==> doc\n",
    "    #if this object with word 'with', return the first token of tokens -- key word \n",
    "    #store children-token if it is noun, return the first token of tokens \n",
    "    \"\"\"\"\n",
    "    children:\n",
    "     A sequence of the token's immediate syntactic children. YIELDS (Token): A child token such that child.head==self\n",
    "    \n",
    "    dep_:\n",
    "       RETURNS (unicode): The syntactic dependency label.\n",
    "       examples:\n",
    "        Bob nsubj\n",
    "        and cc\n",
    "        Mary conj\n",
    "        are aux\n",
    "        taking ROOT\n",
    "        a det\n",
    "        trip dobj\n",
    "        to prep\n",
    "        France pobj\n",
    "        in prep\n",
    "        June pobj\n",
    "        of prep\n",
    "        this det\n",
    "        year pobj\n",
    "        . punct\n",
    "    \"\"\"\"\n",
    "    \n",
    "    if root.lemma_ == 'be' and triplet.object.startswith('friends with'):\n",
    "        fw_doc = nlp(unicode(triplet.object))\n",
    "        with_token = [t for t in fw_doc if t.text == 'with'][0]\n",
    "        fw_who = [t for t in with_token.children if t.dep_ == 'pobj'][0].text\n",
    "        # fw_who = [e for e in fw_doc.ents if e.label_ == 'PERSON'][0].text\n",
    "        #if subject is person, object is person\n",
    "        #add subject and object,\n",
    "        #????\n",
    "        if triplet.subject in [e.text for e in doc.ents if e.label_ == 'PERSON'] and fw_who in [e.text for e in doc.ents if e.label_ == 'PERSON']:\n",
    "            s = add_person(triplet.subject)\n",
    "            o = add_person(fw_who)\n",
    "            s.likes.append(o)\n",
    "            o.likes.append(s)\n",
    "\n",
    "\n",
    "    # Process (PET, has, NAME)\n",
    "    if triplet.subject.endswith('name') and ('dog' in triplet.subject or 'cat' in triplet.subject):\n",
    "        obj_span = doc.char_span(sentence.find(triplet.object), len(sentence))\n",
    "\n",
    "        # handle single names, but what about compound names? Noun chunks might help.\n",
    "        if len(obj_span) == 1 and obj_span[0].pos_ == 'PROPN':\n",
    "            name = triplet.object\n",
    "            subj_start = sentence.find(triplet.subject)\n",
    "            subj_doc = doc.char_span(subj_start, subj_start + len(triplet.subject))\n",
    "\n",
    "            s_people = [token.text for token in subj_doc if token.ent_type_ == 'PERSON']\n",
    "            assert len(s_people) == 1\n",
    "            s_person = select_person(s_people[0])\n",
    "\n",
    "            s_pet_type = 'dog' if 'dog' in triplet.subject else 'cat'\n",
    "\n",
    "            pet = add_pet(s_pet_type, name)\n",
    "\n",
    "            s_person.has.append(pet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- define a function to query question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_question(question):\n",
    "    # remove articles: a, an, the\n",
    "\n",
    "    q_words = question.split(' ')\n",
    "\n",
    "    # when won't this work?\n",
    "    for article in ('a', 'an', 'the'):\n",
    "        try:\n",
    "            q_words.remove(article)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return re.sub(re_spaces, ' ', ' '.join(q_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- define a function to extact information form query question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_question_word(string):\n",
    "    # note: there are other question words\n",
    "    for qword in ('who', 'what'):\n",
    "        if qword in string.lower():\n",
    "            return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- define a main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    sents = get_data_from_file()\n",
    "\n",
    "    cl = ClausIE.get_instance()\n",
    "\n",
    "    triples = cl.extract_triples(sents)\n",
    "\n",
    "    for t in triples:\n",
    "        r = process_relation_triplet(t)\n",
    "        # print(r)\n",
    "\n",
    "    question = ' '\n",
    "    while question[-1] != '?':\n",
    "        question = raw_input(\"Please enter your question: \")\n",
    "\n",
    "        if question[-1] != '?':\n",
    "            print('This is not a question... please try again')\n",
    "\n",
    "    q_trip = cl.extract_triples([preprocess_question(question)])[0]\n",
    "\n",
    "    # (WHO, has, PET)\n",
    "    # here's one just for dogs\n",
    "    if q_trip.subject.lower() == 'who' and q_trip.object == 'dog':\n",
    "        answer = '{} has a {} named {}.'\n",
    "\n",
    "        for person in persons:\n",
    "            pet = get_persons_pet(person.name)\n",
    "            if pet and pet.type == 'dog':\n",
    "                print(answer.format(person.name, 'dog', pet.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
